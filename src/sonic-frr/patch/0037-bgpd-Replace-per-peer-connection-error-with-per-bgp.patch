From 70280ae120823bb005ad19771cf8811f51bf2e70 Mon Sep 17 00:00:00 2001
From: Mark Stapp <mjs@cisco.com>
Date: Thu, 26 Sep 2024 11:09:35 -0400
Subject: [PATCH 37/56] bgpd: Replace per-peer connection error with per-bgp

Replace the per-peer connection error with a per-bgp event and
a list. The io pthread enqueues peers per-bgp-instance, and the
error-handing code can process multiple peers if there have been
multiple failures.

Signed-off-by: Mark Stapp <mjs@cisco.com>
---
 bgpd/bgp_io.c     |  4 +--
 bgpd/bgp_packet.c | 69 +++++++++++++++++++++++++------------
 bgpd/bgpd.c       | 88 +++++++++++++++++++++++++++++++++++++++++++++++
 bgpd/bgpd.h       | 56 +++++++++++++++++++++++++++++-
 4 files changed, 191 insertions(+), 26 deletions(-)

diff --git a/bgpd/bgp_io.c b/bgpd/bgp_io.c
index 5d0f14cc5c..472a5aae59 100644
--- a/bgpd/bgp_io.c
+++ b/bgpd/bgp_io.c
@@ -100,7 +100,6 @@ void bgp_reads_off(struct peer_connection *connection)
 
 	event_cancel_async(fpt->master, &connection->t_read, NULL);
 	EVENT_OFF(connection->t_process_packet);
-	EVENT_OFF(connection->t_process_packet_error);
 
 	UNSET_FLAG(connection->thread_flags, PEER_THREAD_READS_ON);
 }
@@ -252,8 +251,7 @@ static void bgp_process_reads(struct event *thread)
 		/* Handle the error in the main pthread, include the
 		 * specific state change from 'bgp_read'.
 		 */
-		event_add_event(bm->master, bgp_packet_process_error, connection,
-				code, &connection->t_process_packet_error);
+		bgp_enqueue_conn_err_peer(peer->bgp, connection->peer, code);
 		goto done;
 	}
 
diff --git a/bgpd/bgp_packet.c b/bgpd/bgp_packet.c
index 9ebb2b21be..f77c0e5cf8 100644
--- a/bgpd/bgp_packet.c
+++ b/bgpd/bgp_packet.c
@@ -4130,35 +4130,60 @@ void bgp_send_delayed_eor(struct bgp *bgp)
 }
 
 /*
- * Task callback to handle socket error encountered in the io pthread. We avoid
- * having the io pthread try to enqueue fsm events or mess with the peer
- * struct.
+ * Task callback in the main pthread to handle socket error
+ * encountered in the io pthread. We avoid having the io pthread try
+ * to enqueue fsm events or mess with the peer struct.
  */
+
+/* Max number of peers to process without rescheduling */
+#define BGP_CONN_ERROR_DEQUEUE_MAX 10
+
 void bgp_packet_process_error(struct event *thread)
 {
 	struct peer_connection *connection;
 	struct peer *peer;
-	int code;
+	struct bgp *bgp;
+	int counter = 0;
+	bool more_p = false;
 
-	connection = EVENT_ARG(thread);
-	peer = connection->peer;
-	code = EVENT_VAL(thread);
+	bgp = EVENT_ARG(thread);
 
-	if (bgp_debug_neighbor_events(peer))
-		zlog_debug("%s [Event] BGP error %d on fd %d", peer->host, code,
-			   connection->fd);
-
-	/* Closed connection or error on the socket */
-	if (peer_established(connection)) {
-		if ((CHECK_FLAG(peer->flags, PEER_FLAG_GRACEFUL_RESTART)
-		     || CHECK_FLAG(peer->flags,
-				   PEER_FLAG_GRACEFUL_RESTART_HELPER))
-		    && CHECK_FLAG(peer->sflags, PEER_STATUS_NSF_MODE)) {
-			peer->last_reset = PEER_DOWN_NSF_CLOSE_SESSION;
-			SET_FLAG(peer->sflags, PEER_STATUS_NSF_WAIT);
-		} else
-			peer->last_reset = PEER_DOWN_CLOSE_SESSION;
+	/* Dequeue peers from the error list */
+	while ((peer = bgp_dequeue_conn_err_peer(bgp, &more_p)) != NULL) {
+		connection = peer->connection;
+
+		if (bgp_debug_neighbor_events(peer))
+			zlog_debug("%s [Event] BGP error %d on fd %d",
+				   peer->host, peer->connection_errcode,
+				   connection->fd);
+
+		/* Closed connection or error on the socket */
+		if (peer_established(connection)) {
+			if ((CHECK_FLAG(peer->flags, PEER_FLAG_GRACEFUL_RESTART)
+			     || CHECK_FLAG(peer->flags,
+					   PEER_FLAG_GRACEFUL_RESTART_HELPER))
+			    && CHECK_FLAG(peer->sflags, PEER_STATUS_NSF_MODE)) {
+				peer->last_reset = PEER_DOWN_NSF_CLOSE_SESSION;
+				SET_FLAG(peer->sflags, PEER_STATUS_NSF_WAIT);
+			} else
+				peer->last_reset = PEER_DOWN_CLOSE_SESSION;
+		}
+
+		/* No need for keepalives, if enabled */
+		bgp_keepalives_off(connection);
+
+		bgp_event_update(connection, peer->connection_errcode);
+
+		counter++;
+		if (counter >= BGP_CONN_ERROR_DEQUEUE_MAX)
+			break;
 	}
 
-	bgp_event_update(connection, code);
+	/* Reschedule event if necessary */
+	if (more_p)
+		bgp_conn_err_reschedule(bgp);
+
+	if (bgp_debug_neighbor_events(NULL))
+		zlog_debug("%s: dequeued and processed %d peers", __func__,
+			   counter);
 }
diff --git a/bgpd/bgpd.c b/bgpd/bgpd.c
index 6e86bbc070..6bb4e14122 100644
--- a/bgpd/bgpd.c
+++ b/bgpd/bgpd.c
@@ -88,6 +88,9 @@ DEFINE_HOOK(bgp_inst_delete, (struct bgp *bgp), (bgp));
 DEFINE_HOOK(bgp_instance_state, (struct bgp *bgp), (bgp));
 DEFINE_HOOK(bgp_routerid_update, (struct bgp *bgp, bool withdraw), (bgp, withdraw));
 
+/* Peers with connection error/failure, per bgp instance */
+DECLARE_LIST(bgp_peer_conn_errlist, struct peer, conn_err_link);
+
 /* BGP process wide configuration.  */
 static struct bgp_master bgp_master;
 
@@ -2675,6 +2678,9 @@ int peer_delete(struct peer *peer)
 
 	assert(peer->connection->status != Deleted);
 
+	if (bgp_debug_neighbor_events(peer))
+		zlog_debug("%s: peer %pBP", __func__, peer);
+
 	bgp = peer->bgp;
 	accept_peer = CHECK_FLAG(peer->sflags, PEER_STATUS_ACCEPT_PEER);
 
@@ -2690,6 +2696,13 @@ int peer_delete(struct peer *peer)
 			   PEER_THREAD_READS_ON));
 	assert(!CHECK_FLAG(peer->thread_flags, PEER_THREAD_KEEPALIVES_ON));
 
+	/* Ensure the peer is removed from the connection error list */
+	frr_with_mutex (&bgp->peer_errs_mtx) {
+		if (bgp_peer_conn_errlist_anywhere(peer))
+			bgp_peer_conn_errlist_del(&bgp->peer_conn_errlist,
+						  peer);
+	}
+
 	if (CHECK_FLAG(peer->sflags, PEER_STATUS_NSF_WAIT))
 		peer_nsf_stop(peer);
 
@@ -3610,6 +3623,10 @@ peer_init:
 	memset(&bgp->ebgprequirespolicywarning, 0,
 	       sizeof(bgp->ebgprequirespolicywarning));
 
+	/* Init peer connection error info */
+	pthread_mutex_init(&bgp->peer_errs_mtx, NULL);
+	bgp_peer_conn_errlist_init(&bgp->peer_conn_errlist);
+
 	return bgp;
 }
 
@@ -4166,6 +4183,18 @@ int bgp_delete(struct bgp *bgp)
 			if (i != ZEBRA_ROUTE_BGP)
 				bgp_redistribute_unset(bgp, afi, i, 0);
 
+	/* Clear list of peers with connection errors - each
+	 * peer will need to check again, in case the io pthread is racing
+	 * with us, but this batch cleanup should make the per-peer check
+	 * cheaper.
+	 */
+	frr_with_mutex (&bgp->peer_errs_mtx) {
+		do {
+			peer = bgp_peer_conn_errlist_pop(
+				&bgp->peer_conn_errlist);
+		} while (peer != NULL);
+	}
+
 	/* Free peers and peer-groups. */
 	for (ALL_LIST_ELEMENTS(bgp->group, node, next, group))
 		peer_group_delete(group);
@@ -4182,6 +4211,9 @@ int bgp_delete(struct bgp *bgp)
 
 	update_bgp_group_free(bgp);
 
+	/* Cancel peer connection errors event */
+	EVENT_OFF(bgp->t_conn_errors);
+
 /* TODO - Other memory may need to be freed - e.g., NHT */
 
 #ifdef ENABLE_BGP_VNC
@@ -4357,6 +4389,9 @@ void bgp_free(struct bgp *bgp)
 	bgp_srv6_cleanup(bgp);
 	bgp_confederation_id_unset(bgp);
 
+	bgp_peer_conn_errlist_init(&bgp->peer_conn_errlist);
+	pthread_mutex_destroy(&bgp->peer_errs_mtx);
+
 	for (int i = 0; i < bgp->confed_peers_cnt; i++)
 		XFREE(MTYPE_BGP_NAME, bgp->confed_peers[i].as_pretty);
 
@@ -8946,6 +8981,59 @@ void bgp_gr_apply_running_config(void)
 	}
 }
 
+/*
+ * Enqueue a peer with a connection error to be handled in the main pthread
+ */
+int bgp_enqueue_conn_err_peer(struct bgp *bgp, struct peer *peer, int errcode)
+{
+	frr_with_mutex (&bgp->peer_errs_mtx) {
+		peer->connection_errcode = errcode;
+
+		/* Careful not to double-enqueue */
+		if (!bgp_peer_conn_errlist_anywhere(peer)) {
+			bgp_peer_conn_errlist_add_tail(&bgp->peer_conn_errlist,
+						       peer);
+		}
+	}
+	/* Ensure an event is scheduled */
+	event_add_event(bm->master, bgp_packet_process_error, bgp, 0,
+			&bgp->t_conn_errors);
+	return 0;
+}
+
+/*
+ * Dequeue a peer that encountered a connection error; signal whether there
+ * are more queued peers.
+ */
+struct peer *bgp_dequeue_conn_err_peer(struct bgp *bgp, bool *more_p)
+{
+	struct peer *peer = NULL;
+	bool more = false;
+
+	frr_with_mutex (&bgp->peer_errs_mtx) {
+		peer = bgp_peer_conn_errlist_pop(&bgp->peer_conn_errlist);
+
+		if (bgp_peer_conn_errlist_const_first(
+			    &bgp->peer_conn_errlist) != NULL)
+			more = true;
+	}
+
+	if (more_p)
+		*more_p = more;
+
+	return peer;
+}
+
+/*
+ * Reschedule the connection error event - probably after processing
+ * some of the peers on the list.
+ */
+void bgp_conn_err_reschedule(struct bgp *bgp)
+{
+	event_add_event(bm->master, bgp_packet_process_error, bgp, 0,
+			&bgp->t_conn_errors);
+}
+
 printfrr_ext_autoreg_p("BP", printfrr_bp);
 static ssize_t printfrr_bp(struct fbuf *buf, struct printfrr_eargs *ea,
 			   const void *ptr)
diff --git a/bgpd/bgpd.h b/bgpd/bgpd.h
index ee904391c1..16c9fc54f4 100644
--- a/bgpd/bgpd.h
+++ b/bgpd/bgpd.h
@@ -385,6 +385,33 @@ struct as_confed {
 struct bgp_mplsvpn_nh_label_bind_cache;
 PREDECL_RBTREE_UNIQ(bgp_mplsvpn_nh_label_bind_cache);
 
+/* List of peers that have connection errors in the io pthread */
+PREDECL_LIST(bgp_peer_conn_errlist);
+
+/* List of info about peers that are being cleared from BGP RIBs in a batch */
+PREDECL_LIST(bgp_clearing_info);
+
+/* Hash of peers in clearing info object */
+PREDECL_HASH(bgp_clearing_hash);
+
+/* Info about a batch of peers that need to be cleared from the RIB.
+ * If many peers need to be cleared, we process them in batches, taking
+ * one walk through the RIB for each batch.
+ */
+struct bgp_clearing_info {
+	/* Hash of peers */
+	struct bgp_clearing_hash_head peers;
+
+	/* Event to schedule/reschedule processing */
+	struct thread *t_sched;
+
+	/* RIB dest for rescheduling */
+	struct bgp_dest *last_dest;
+
+	/* Linkage for list of batches per-bgp */
+	struct bgp_clearing_info_item link;
+};
+
 /* BGP instance structure.  */
 struct bgp {
 	/* AS number of this BGP instance.  */
@@ -869,6 +896,21 @@ struct bgp {
 	uint16_t tcp_keepalive_intvl;
 	uint16_t tcp_keepalive_probes;
 
+	/* List of peers that have connection errors in the IO pthread */
+	struct bgp_peer_conn_errlist_head peer_conn_errlist;
+
+	/* Mutex that guards the connection-errors list */
+	pthread_mutex_t peer_errs_mtx;
+
+	/* Event indicating that there have been connection errors; this
+	 * is typically signalled in the IO pthread; it's handled in the
+	 * main pthread.
+	 */
+	struct event *t_conn_errors;
+
+	/* List of batches of peers being cleared from BGP RIBs */
+	struct bgp_clearing_info_head clearing_list;
+
 	struct timeval ebgprequirespolicywarning;
 #define FIFTEENMINUTE2USEC (int64_t)15 * 60 * 1000000
 
@@ -1251,7 +1293,6 @@ struct peer_connection {
 
 	struct event *t_routeadv;
 	struct event *t_process_packet;
-	struct event *t_process_packet_error;
 
 	struct event *t_stop_with_notify;
 
@@ -1940,6 +1981,15 @@ struct peer {
 	/* Add-Path Paths-Limit */
 	struct addpath_paths_limit addpath_paths_limit[AFI_MAX][SAFI_MAX];
 
+	/* Linkage for list of peers with connection errors from IO pthread */
+	struct bgp_peer_conn_errlist_item conn_err_link;
+
+	/* Connection error code */
+	uint16_t connection_errcode;
+
+	/* Linkage for hash of clearing peers being cleared in a batch */
+	struct bgp_clearing_hash_item clear_hash_link;
+
 	QOBJ_FIELDS;
 };
 DECLARE_QOBJ_TYPE(peer);
@@ -2579,6 +2629,10 @@ void bgp_gr_apply_running_config(void);
 int bgp_global_gr_init(struct bgp *bgp);
 int bgp_peer_gr_init(struct peer *peer);
 
+/* APIs for the per-bgp peer connection error list */
+int bgp_enqueue_conn_err_peer(struct bgp *bgp, struct peer *peer, int errcode);
+struct peer *bgp_dequeue_conn_err_peer(struct bgp *bgp, bool *more_p);
+void bgp_conn_err_reschedule(struct bgp *bgp);
 
 #define BGP_GR_ROUTER_DETECT_AND_SEND_CAPABILITY_TO_ZEBRA(_bgp, _peer_list)    \
 	do {                                                                   \
-- 
2.39.5

